{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas transformers torch scikit-learn nltk datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDlRcoKxz7WT",
        "outputId": "27695005-7cfb-46a8-dad9-e962070a5d59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DvOxDPAdDc4",
        "outputId": "66fa7ec1-6105-4ac4-c7c2-b36920d354c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Common N-grams:\n",
            "group project: 9\n",
            "storming stage,: 9\n",
            "storming stage: 9\n",
            "impact project: 5\n",
            "within team.: 4\n",
            "group also: 4\n",
            "group members: 4\n",
            "gave us: 3\n",
            "team members: 3\n",
            "different perspectives: 3\n",
            "Top Keywords based on TF-IDF:\n",
            "Document 1: ['patients', 'optimism', 'art', 'hope', 'sense']\n",
            "Document 2: ['future', 'us', 'well', 'accomplishment', 'acquire']\n",
            "Document 3: ['program', 'drawings', 'form', 'could', 'also']\n",
            "Document 4: ['goals', 'success', 'chosen', 'plans', 'recipients']\n",
            "Document 5: ['history', 'bombers', 'heavy', 'impact', 'collaborated']\n",
            "Document 6: ['elderly', 'being', 'improving', 'well', 'aspects']\n",
            "Document 7: ['stage', 'group', 'day', 'absent', 'accepting']\n",
            "Document 8: ['decision', 'without', 'group', 'member', 'make']\n",
            "Document 9: ['others', 'better', 'environment', 'positive', 'within']\n",
            "Document 10: ['level', 'team', 'experience', 'captured', 'characters']\n",
            "Document 11: ['comes', 'goal', 'solving', 'problem', 'others']\n",
            "Document 12: ['children', 'made', 'acknowledge', 'battles', 'cheerful']\n",
            "Document 13: ['activity', 'game', 'since', 'other', 'become']\n",
            "Document 14: ['podcast', 'impact', '4th', 'capable', 'episodes']\n",
            "Document 15: ['varied', 'collaboration', 'goal', 'individual', 'communication']\n",
            "Document 16: ['working', 'group', 'alone', 'completion', 'depends']\n",
            "Document 17: ['good', 'admit', 'doubt', 'pressure', 'pushed']\n",
            "Document 18: ['art', 'therapy', 'recipients', 'individuals', 'express']\n",
            "Document 19: ['film', 'project', 'start', 'made', 'impact']\n",
            "Document 20: ['discussions', 'tough', 'personally', 'times', 'suggestions']\n",
            "Document 21: ['heaven', 'chat', 'would', 'kept', 'group']\n",
            "Document 22: ['we', 've', 'opinions', 'plan', 'comments']\n",
            "Document 23: ['everything', 'phase', 'managed', 'case', 'far']\n",
            "Document 24: ['group', 'professor', 'project', 'idea', 'help']\n",
            "Document 25: ['agreement', 'furthermore', 'group', 'regarding', 'anticipate']\n",
            "Document 26: ['yet', 'beginning', 'lapses', 'right', 'able']\n",
            "Document 27: ['podcast', 'meeting', 'everyone', 'task', 'week']\n",
            "Document 28: ['want', 'gives', 'decisions', 'since', 'hope']\n",
            "Document 29: ['team', 'stage', 'member', 'development', 'another']\n",
            "Document 30: ['feel', 'group', 'stage', 'say', 'make']\n",
            "Document 31: ['ms', 'salazar', 'meeting', 'aside', 'unresponsive']\n",
            "Document 32: ['everyone', 'yet', 'bit', 'observe', 'teammate']\n",
            "Document 33: ['teamwork', 'team', 'role', 'within', 'arise']\n",
            "Document 34: ['need', 'project', 'could', 'honestly', 'us']\n",
            "Document 35: ['principal', 'school', 'us', 'time', 'office']\n",
            "Document 36: ['abandoned', 'orphanage', 'locate', 'elderly', 'decided']\n",
            "Document 37: ['face', 'now', 'stage', 'group', 'storming']\n",
            "Document 38: ['project', 'good', 'learning', 'stage', 'it']\n",
            "Extracted themes and keywords saved to 'extracted_themes_and_keywords.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Download stopwords if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/Leader Reflections.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Assuming the text data is in a column named 'Reflection/Experience'\n",
        "text_data = data['Reflection/Experience'].astype(str).tolist()\n",
        "\n",
        "# Get English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to remove stopwords from text\n",
        "def remove_stopwords(text):\n",
        "    return ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
        "\n",
        "# Function to extract n-grams\n",
        "def extract_ngrams(text, n):\n",
        "    tokens = text.split()\n",
        "    return list(ngrams(tokens, n))\n",
        "\n",
        "# Remove stopwords from each text entry\n",
        "cleaned_text_data = [remove_stopwords(text) for text in text_data]\n",
        "\n",
        "# Extracting n-grams for each cleaned text entry\n",
        "n = 2\n",
        "all_ngrams = [extract_ngrams(text, n) for text in cleaned_text_data]\n",
        "\n",
        "# Flatten the list of n-grams and count them\n",
        "flat_ngrams = [ngram for sublist in all_ngrams for ngram in sublist]\n",
        "ngram_counts = Counter(flat_ngrams)\n",
        "\n",
        "# Display the most common n-grams\n",
        "print(\"Most Common N-grams:\")\n",
        "for ngram, count in ngram_counts.most_common(10):\n",
        "    print(f\"{' '.join(ngram)}: {count}\")\n",
        "\n",
        "# Create a TF-IDF Vectorizer and fit it to the cleaned text data\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(cleaned_text_data)\n",
        "\n",
        "# Get feature names (words)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Create a DataFrame for TF-IDF scores\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
        "\n",
        "# Display the top keywords based on TF-IDF scores for each document\n",
        "print(\"Top Keywords based on TF-IDF:\")\n",
        "for index, row in tfidf_df.iterrows():\n",
        "    top_keywords = row.nlargest(5)  # Get top 5 keywords\n",
        "    print(f\"Document {index + 1}: {top_keywords.index.tolist()}\")\n",
        "\n",
        "# Save extracted themes and keywords to CSV\n",
        "output_df = pd.DataFrame({\n",
        "    'Document': range(1, len(cleaned_text_data) + 1),\n",
        "    'Top Keywords': [row.nlargest(5).index.tolist() for _, row in tfidf_df.iterrows()]\n",
        "})\n",
        "\n",
        "output_df.to_csv('extracted_themes_and_keywords.csv', index=False)\n",
        "print(\"Extracted themes and keywords saved to 'extracted_themes_and_keywords.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Leader's Role in Group Development categories and their associated behaviors\n",
        "leader_roles = {\n",
        "    \"Leadership Role\": {\n",
        "        \"Commonly Shared Behaviors\": [\n",
        "            \"Guidance and Direction\",\n",
        "            \"Conflict Management\",\n",
        "            \"Team Cohesion\",\n",
        "            \"Motivating and Inspiring\",\n",
        "            \"Participation Encouragement\",\n",
        "            \"Effective Communication\",\n",
        "            \"Personal and Team Development Support\",\n",
        "            \"Recognition and Appreciation\"\n",
        "        ],\n",
        "        \"Experiencing Leadership Role\": [\n",
        "            \"Clear Instructions\",\n",
        "            \"Mediating Conflicts\",\n",
        "            \"Building Trust\",\n",
        "            \"Positive Reinforcement\",\n",
        "            \"Active Involvement\",\n",
        "            \"Open Communication Channels\",\n",
        "            \"Encouraging Skill Improvement\",\n",
        "            \"Acknowledging Contributions\"\n",
        "        ],\n",
        "        \"Celebrating Success\": [\n",
        "            \"Setting Expectations\",\n",
        "            \"Handling Disagreements\",\n",
        "            \"Promoting Unity\",\n",
        "            \"Boosting Team Morale\",\n",
        "            \"Ensuring Engagement\",\n",
        "            \"Addressing Miscommunication\",\n",
        "            \"Supporting Member Growth\",\n",
        "            \"Regular Positive Feedback\"\n",
        "        ],\n",
        "        \"New Insights\": [\n",
        "            \"Productive Team Performance\",\n",
        "            \"Overcoming Challenges\",\n",
        "            \"Group Achievement\",\n",
        "            \"High Morale and Motivation\",\n",
        "            \"Celebrating Team Contributions\",\n",
        "            \"Successful Team Discussions\",\n",
        "            \"Recognition of Individual Progress\",\n",
        "            \"Celebrating Milestones\"\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Convert the structured data into a DataFrame for easier manipulation\n",
        "data_roles = []\n",
        "for category, subcategories in leader_roles.items():\n",
        "    for subcategory, behaviors in subcategories.items():\n",
        "        for behavior in behaviors:\n",
        "            data_roles.append({\n",
        "                'Category': category,\n",
        "                'Subcategory': subcategory,\n",
        "                'Behavior': behavior\n",
        "            })\n",
        "\n",
        "df_roles = pd.DataFrame(data_roles)\n",
        "\n",
        "# Display the DataFrame of leadership roles\n",
        "print(df_roles)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "output_file_path = 'leaders_role_in_group_development.csv'\n",
        "df_roles.to_csv(output_file_path, index=False)\n",
        "print(f\"Data saved to {output_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96JNNo3o98S4",
        "outputId": "f49a6834-9ec1-45a4-9186-266e680712a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Category                   Subcategory  \\\n",
            "0   Leadership Role     Commonly Shared Behaviors   \n",
            "1   Leadership Role     Commonly Shared Behaviors   \n",
            "2   Leadership Role     Commonly Shared Behaviors   \n",
            "3   Leadership Role     Commonly Shared Behaviors   \n",
            "4   Leadership Role     Commonly Shared Behaviors   \n",
            "5   Leadership Role     Commonly Shared Behaviors   \n",
            "6   Leadership Role     Commonly Shared Behaviors   \n",
            "7   Leadership Role     Commonly Shared Behaviors   \n",
            "8   Leadership Role  Experiencing Leadership Role   \n",
            "9   Leadership Role  Experiencing Leadership Role   \n",
            "10  Leadership Role  Experiencing Leadership Role   \n",
            "11  Leadership Role  Experiencing Leadership Role   \n",
            "12  Leadership Role  Experiencing Leadership Role   \n",
            "13  Leadership Role  Experiencing Leadership Role   \n",
            "14  Leadership Role  Experiencing Leadership Role   \n",
            "15  Leadership Role  Experiencing Leadership Role   \n",
            "16  Leadership Role           Celebrating Success   \n",
            "17  Leadership Role           Celebrating Success   \n",
            "18  Leadership Role           Celebrating Success   \n",
            "19  Leadership Role           Celebrating Success   \n",
            "20  Leadership Role           Celebrating Success   \n",
            "21  Leadership Role           Celebrating Success   \n",
            "22  Leadership Role           Celebrating Success   \n",
            "23  Leadership Role           Celebrating Success   \n",
            "24  Leadership Role                  New Insights   \n",
            "25  Leadership Role                  New Insights   \n",
            "26  Leadership Role                  New Insights   \n",
            "27  Leadership Role                  New Insights   \n",
            "28  Leadership Role                  New Insights   \n",
            "29  Leadership Role                  New Insights   \n",
            "30  Leadership Role                  New Insights   \n",
            "31  Leadership Role                  New Insights   \n",
            "\n",
            "                                 Behavior  \n",
            "0                  Guidance and Direction  \n",
            "1                     Conflict Management  \n",
            "2                           Team Cohesion  \n",
            "3                Motivating and Inspiring  \n",
            "4             Participation Encouragement  \n",
            "5                 Effective Communication  \n",
            "6   Personal and Team Development Support  \n",
            "7            Recognition and Appreciation  \n",
            "8                      Clear Instructions  \n",
            "9                     Mediating Conflicts  \n",
            "10                         Building Trust  \n",
            "11                 Positive Reinforcement  \n",
            "12                     Active Involvement  \n",
            "13            Open Communication Channels  \n",
            "14          Encouraging Skill Improvement  \n",
            "15            Acknowledging Contributions  \n",
            "16                   Setting Expectations  \n",
            "17                 Handling Disagreements  \n",
            "18                        Promoting Unity  \n",
            "19                   Boosting Team Morale  \n",
            "20                    Ensuring Engagement  \n",
            "21            Addressing Miscommunication  \n",
            "22               Supporting Member Growth  \n",
            "23              Regular Positive Feedback  \n",
            "24            Productive Team Performance  \n",
            "25                  Overcoming Challenges  \n",
            "26                      Group Achievement  \n",
            "27             High Morale and Motivation  \n",
            "28         Celebrating Team Contributions  \n",
            "29            Successful Team Discussions  \n",
            "30     Recognition of Individual Progress  \n",
            "31                 Celebrating Milestones  \n",
            "Data saved to leaders_role_in_group_development.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def extract_sentences_with_keywords(text, keywords):\n",
        "    # Preprocess text to ensure proper sentence splitting\n",
        "    doc = nlp(text)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents]\n",
        "\n",
        "    # Dictionary to store sentence-keywords pairs\n",
        "    sentence_keywords = {}\n",
        "\n",
        "    # Look for keywords in sentences\n",
        "    for sentence in sentences:\n",
        "        found_keywords = []\n",
        "        for keyword in keywords:\n",
        "            # Create pattern that matches the keyword (including word boundaries)\n",
        "            pattern = r'\\b' + re.escape(keyword) + r'\\b'\n",
        "            if re.search(pattern, sentence.lower()):\n",
        "                found_keywords.append(keyword)\n",
        "\n",
        "        # If keywords were found in this sentence\n",
        "        if found_keywords:\n",
        "            sentence_keywords[sentence] = found_keywords\n",
        "\n",
        "    return sentence_keywords\n",
        "\n",
        "# Define your original keywords\n",
        "keywords = [\n",
        "    'patients', 'optimism', 'art', 'hope', 'sense',\n",
        "'future', 'us', 'well', 'accomplishment', 'acquire',\n",
        "'program', 'drawings', 'form', 'could', 'also',\n",
        "'goals', 'success', 'chosen', 'plans', 'recipients',\n",
        "'history', 'bombers', 'heavy', 'impact', 'collaborated',\n",
        "'elderly', 'being', 'improving', 'well', 'aspects',\n",
        "'stage', 'group', 'day', 'absent', 'accepting',\n",
        "'decision', 'without', 'group', 'member', 'make',\n",
        "'others', 'better', 'environment', 'positive', 'within',\n",
        "'level', 'team', 'experience', 'captured', 'characters',\n",
        "'comes', 'goal', 'solving', 'problem', 'others',\n",
        "'children', 'made', 'acknowledge', 'battles', 'cheerful',\n",
        "'activité’, ‘game’, ‘since’, ‘other’, ‘become’,\n",
        "'podcast', 'impact', '4th', 'capable', 'episodes',\n",
        "'varied', 'collaboration', 'goal', 'individual', 'communication',\n",
        "'working’, ‘group’, ‘alone’, ‘completion’, ‘depends’,\n",
        "'good’, ‘admit’, ‘doubt’, ‘pressure’, ‘pushed’,\n",
        "'art’, ‘therapy’, ‘recipients’, ‘individuals’, ‘express’,\n",
        "'film’, ‘project’, ‘start’, ‘made’, ‘impact’,\n",
        "'discussions’, ‘tough’, ‘personally’, ‘times’, ‘suggestions’,\n",
        "'heaven’, ‘chat’, ‘would’, ‘kept’, ‘group’,\n",
        "'we’, ‘ve’, ‘opinions’, ‘plan’, ‘comments’,\n",
        "'everything’, ’phase’, ’managed’, ’case’, ’far’,\n",
        "'group’, ’professor’, ’project’, ’idea’, ’help’,\n",
        "'agreement’, ’furthermore’, ’group’, ’regarding’, ’anticipate’,\n",
        "'yet,’ ’beginning,’ ’lapses,’ ’right,’ ’able’,\n",
        "'podcast,’ ’meeting,’ ’everyone,’ ’task,’ ’week’,\n",
        "'want,’ ’gives,’ ’decisions,’ ’since,’ ’hope’,\n",
        "'team,’ ’stage,’ ’member,’ ’development,’ ’another’,\n",
        "'feel,' ’group,' ’stage,' ’say,' ’make’,\n",
        "'ms,' ’salazar,' ’meeting,' ’aside,' ’unresponsive’,\n",
        "'everyone,' ’yet,' ’bit,' ’observe,' ’teammate’,\n",
        "'teamwork,' ’team,' ’role,' ’within,' ’arise’,\n",
        "'need,' ’project,' ’could,' ’honestly,' ’us’,\n",
        "'principal,' ’school,’ ’us,’  ’time,’  ’officer’,\n",
        "'abandoned,'  ’orphanage,’  ’locate,’  ’elderly,’  ’decided’,\n",
        "'face,'  ’now,’  ‘stage,’  ‘group,’  ‘storming’,\n",
        "'project,'  ‘good,’  ‘learning,’  ‘stage,’  ‘it’\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "# Read the Storming_Analysis Excel file\n",
        "df = pd.read_csv('/content/Leader Reflections.csv')\n",
        "\n",
        "# Process each row for original keywords\n",
        "results = []\n",
        "for index, row in df.iterrows():\n",
        "    participant = f\"Participant {index + 1}\"\n",
        "    text = row['Reflection/Experience']\n",
        "\n",
        "    # Extract sentences with keywords\n",
        "    extracted = extract_sentences_with_keywords(text, keywords)\n",
        "\n",
        "    # Add to results\n",
        "    for sentence, found_keywords in extracted.items():\n",
        "        results.append({\n",
        "            'Participant': participant,\n",
        "            'Keywords': ', '.join(found_keywords),\n",
        "            'Sentence': sentence\n",
        "        })\n",
        "\n",
        "# Process each row for additional keywords\n",
        "literature_results = []\n",
        "for index, row in df.iterrows():\n",
        "    participant = f\"Participant {index + 1}\"\n",
        "    text = row['Reflection/Experience']\n",
        "\n",
        "    # Extract sentences with additional keywords\n",
        "    extracted = extract_sentences_with_keywords(text, additional_keywords)\n",
        "\n",
        "    # Add to results if keywords were found\n",
        "    for sentence, found_keywords in extracted.items():\n",
        "        literature_results.append({\n",
        "            'Participant': participant,\n",
        "            'Literature_Keywords': ', '.join(found_keywords),\n",
        "            'Sentence': sentence\n",
        "        })\n",
        "\n",
        "# Create output dataframes\n",
        "output_df = pd.DataFrame(results)\n",
        "literature_df = pd.DataFrame(literature_results)\n",
        "\n",
        "# Create Excel writer object\n",
        "with pd.ExcelWriter('Leader_analysis_results.xlsx', engine='openpyxl') as writer:\n",
        "    # Write the main analysis to the first sheet\n",
        "    output_df.to_excel(writer, sheet_name='Analysis Results', index=False)\n",
        "\n",
        "    # Write the literature keywords results to the second sheet (only if there are results)\n",
        "    if not literature_df.empty:\n",
        "        literature_df.to_excel(writer, sheet_name='Literature Keywords', index=False)\n",
        "\n",
        "print(\"Analysis completed and saved to Excel file.\")"
      ],
      "metadata": {
        "id": "aVQyEiXhfpk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/Leader Reflections.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Check columns and inspect data\n",
        "print(data.columns)\n",
        "\n",
        "# Create labels manually based on the 'Reflection/Experience' content\n",
        "labels = [0 if 'leadership' in text.lower() else 1 for text in data['Reflection/Experience']]\n",
        "\n",
        "# Split data into training and test sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(data['Reflection/Experience'].astype(str).tolist(), labels, test_size=0.2)\n",
        "\n",
        "# Load BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the text data\n",
        "def tokenize_function(texts):\n",
        "    return tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
        "\n",
        "train_encodings = tokenize_function(train_texts)\n",
        "test_encodings = tokenize_function(test_texts)\n",
        "\n",
        "# Convert the encoded data to PyTorch datasets\n",
        "class LeaderDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Create the dataset objects\n",
        "train_dataset = LeaderDataset(train_encodings, train_labels)\n",
        "test_dataset = LeaderDataset(test_encodings, test_labels)\n",
        "\n",
        "# Load the pre-trained BERT model for sequence classification\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # number of training epochs\n",
        "    per_device_train_batch_size=8,   # batch size for training\n",
        "    per_device_eval_batch_size=8,    # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        "    report_to=\"none\"  # Disable Weights & Biases logging\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset,           # evaluation dataset\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "predictions, label_ids, _ = trainer.predict(test_dataset)\n",
        "predicted_labels = torch.argmax(torch.tensor(predictions), dim=1)\n",
        "\n",
        "# Generate classification report\n",
        "print(classification_report(test_labels, predicted_labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "cAM4edIUK0vD",
        "outputId": "da039c04-ac46-4ea8-d9b9-42f6979db754"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Sections', 'Groups', 'Leader', 'Reflection/Experience', 'Unnamed: 4',\n",
            "       'Unnamed: 5'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-17-ac5e21568e49>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [12/12 10:52, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.649000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-ac5e21568e49>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      1.00      1.00         8\n",
            "\n",
            "    accuracy                           1.00         8\n",
            "   macro avg       1.00      1.00      1.00         8\n",
            "weighted avg       1.00      1.00      1.00         8\n",
            "\n"
          ]
        }
      ]
    }
  ]
}